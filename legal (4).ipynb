{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gc\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom transformers import (\n    AutoTokenizer, BertForMaskedLM, DistilBertForMaskedLM,\n    DistilBertForSequenceClassification, DistilBertForTokenClassification,\n    TrainingArguments, Trainer\n)\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nimport re\nimport ast\nimport json\nimport os\nfrom kaggle_secrets import UserSecretsClient\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW\n\n# Download NLTK data for sentence tokenization\nnltk.download('punkt', quiet=True)\n\n# Create directories\nos.makedirs(\"/kaggle/working/data/processed\", exist_ok=True)\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\n\n# Set device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\ndef preprocess_data():\n    # Securely load Hugging Face token\n    user_secrets = UserSecretsClient()\n    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n    \n    if not HF_TOKEN:\n        raise ValueError(\"Hugging Face token not found in Kaggle secrets. \"\n                         \"Please add your HF_TOKEN to Kaggle secrets.\")\n    \n    # Authenticate with Hugging Face\n    login(token=HF_TOKEN)\n    \n    # Load CJPE dataset\n    print(\"Loading CJPE dataset...\")\n    dataset = load_dataset(\"Exploration-Lab/IL-TUR\", \"cjpe\")\n    print(\"Dataset loaded successfully!\")\n    \n    # Convert to DataFrames\n    df_train = pd.DataFrame(dataset[\"single_train\"])\n    df_dev = pd.DataFrame(dataset[\"single_dev\"])\n    df_test = pd.DataFrame(dataset[\"test\"])\n    df_expert = pd.DataFrame(dataset[\"expert\"]) if \"expert\" in dataset else None\n    \n    # Create raw_text column for all splits\n    df_train[\"raw_text\"] = df_train[\"text\"]\n    df_dev[\"raw_text\"] = df_dev[\"text\"]\n    df_test[\"raw_text\"] = df_test[\"text\"]\n    \n    # Add raw_text for expert split if it exists\n    if df_expert is not None:\n        df_expert[\"raw_text\"] = df_expert[\"text\"]\n    \n    # Tokenization\n    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    MAX_LEN = 512\n    \n    def tokenize_batch(texts):\n        return tokenizer(\n            texts,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN,\n            return_tensors=\"pt\",\n            return_offsets_mapping=True\n        )\n    \n    print(\"Tokenizing data...\")\n    train_tokens = tokenize_batch(df_train[\"raw_text\"].tolist())\n    dev_tokens = tokenize_batch(df_dev[\"raw_text\"].tolist())\n    test_tokens = tokenize_batch(df_test[\"raw_text\"].tolist())\n    \n    # Initialize rationale masks with zeros\n    df_train[\"rationale_mask\"] = [[0]*MAX_LEN for _ in range(len(df_train))]\n    df_dev[\"rationale_mask\"] = [[0]*MAX_LEN for _ in range(len(df_dev))]\n    \n    # Process expert split for rationales\n    if df_expert is not None:\n        print(\"Processing expert annotations for rationale extraction...\")\n        df_expert[\"rationale_mask\"] = [[0]*MAX_LEN for _ in range(len(df_expert))]\n        expert_tokens = tokenize_batch(df_expert[\"raw_text\"].tolist())\n        \n        for idx in tqdm(range(len(df_expert)), desc=\"Expert rationale masks\"):\n            row = df_expert.iloc[idx]\n            text = row[\"raw_text\"]\n            offsets = expert_tokens[\"offset_mapping\"][idx].tolist()\n            \n            # Extract sentences using NLTK\n            sentences = sent_tokenize(text)\n            \n            # Collect expert rationale sentences\n            expert_sentences = set()\n            for i in range(1, 6):\n                expert = row.get(f\"expert_{i}\")\n                if not expert:\n                    continue\n                \n                # Handle different annotation formats\n                if isinstance(expert, str):\n                    try:\n                        # Try to parse as Python literal\n                        expert = ast.literal_eval(expert)\n                    except (ValueError, SyntaxError):\n                        try:\n                            # Try to parse as JSON\n                            expert = json.loads(expert)\n                        except json.JSONDecodeError:\n                            # Skip if both parsing methods fail\n                            continue\n                \n                # Extract sentences from expert annotations\n                for rank in ['rank1', 'rank2', 'rank3', 'rank4', 'rank5']:\n                    if rank in expert:\n                        sentences_list = expert[rank]\n                        if isinstance(sentences_list, str):\n                            try:\n                                # Parse string representation of list\n                                sentences_list = ast.literal_eval(sentences_list)\n                            except (ValueError, SyntaxError):\n                                continue\n                        if isinstance(sentences_list, list):\n                            # Add cleaned sentences to the set\n                            expert_sentences.update([s.strip() for s in sentences_list])\n            \n            # Create rationale mask\n            mask = [0] * MAX_LEN\n            for sent in sentences:\n                if sent.strip() in expert_sentences:\n                    # Find all occurrences of the sentence\n                    pattern = re.escape(sent)\n                    for match in re.finditer(pattern, text):\n                        start_idx = match.start()\n                        end_idx = match.end()\n                        \n                        # Mark tokens within rationale span\n                        for i, (start, end) in enumerate(offsets):\n                            if i >= MAX_LEN:\n                                break\n                            if start == 0 and end == 0:  # Skip special tokens\n                                continue\n                            if not (end <= start_idx or start >= end_idx):\n                                mask[i] = 1\n            \n            df_expert.at[idx, \"rationale_mask\"] = mask\n        \n        # Save expert data\n        df_expert.to_csv(\"/kaggle/working/data/processed/expert.csv\", index=False)\n        torch.save(expert_tokens, \"/kaggle/working/data/processed/expert_tokens.pt\")\n    \n    # Save processed data\n    OUTPUT_DIR = \"/kaggle/working/data/processed\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    df_train.to_csv(f\"{OUTPUT_DIR}/train.csv\", index=False)\n    df_dev.to_csv(f\"{OUTPUT_DIR}/dev.csv\", index=False)\n    df_test.to_csv(f\"{OUTPUT_DIR}/test.csv\", index=False)\n    \n    torch.save({\n        \"input_ids\": train_tokens[\"input_ids\"],\n        \"attention_mask\": train_tokens[\"attention_mask\"]\n    }, f\"{OUTPUT_DIR}/train_tokens.pt\")\n    \n    torch.save({\n        \"input_ids\": dev_tokens[\"input_ids\"],\n        \"attention_mask\": dev_tokens[\"attention_mask\"],\n        \"offset_mapping\": dev_tokens[\"offset_mapping\"]\n    }, f\"{OUTPUT_DIR}/dev_tokens.pt\")\n    \n    torch.save({\n        \"input_ids\": test_tokens[\"input_ids\"],\n        \"attention_mask\": test_tokens[\"attention_mask\"],\n        \"offset_mapping\": test_tokens[\"offset_mapping\"]\n    }, f\"{OUTPUT_DIR}/test_tokens.pt\")\n    \n    print(f\"Preprocessing complete! Files saved to {OUTPUT_DIR}\")\n    \n    # Calculate rationale coverage for expert split\n    if df_expert is not None:\n        def calculate_coverage(masks):\n            total_tokens = 0\n            positive_tokens = 0\n            for m in masks:\n                # Only consider actual text tokens (ignore padding)\n                valid_tokens = len([x for x in m if x != -1])\n                total_tokens += valid_tokens\n                positive_tokens += sum(m[:valid_tokens])\n            coverage = positive_tokens / total_tokens if total_tokens > 0 else 0\n            return coverage\n        \n        expert_coverage = calculate_coverage(df_expert[\"rationale_mask\"])\n        print(f\"Expert rationale coverage: {expert_coverage:.4%}\")\n    \n    return df_train, df_dev, df_test, df_expert if df_expert is not None else None\n\n# Execute preprocessing\ntrain_df, dev_df, test_df, expert_df = preprocess_data()\n\n# Define ClassificationDataset\nclass ClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Step 2: Corrected Distillation Training\ndef distillation_training():\n    # Configuration\n    TEACHER_MODEL = \"bert-base-uncased\"\n    STUDENT_MODEL = \"distilbert-base-uncased\"\n    DATA_PATH = \"/kaggle/working/data/processed/train_tokens.pt\"\n    SAVE_PATH = \"/kaggle/working/models/distilled_model\"\n    BATCH_SIZE = 8\n    EPOCHS = 1\n    ALPHA, BETA, GAMMA = 0.5, 0.4, 0.1  # Adjusted loss weights\n    \n    # Load token data\n    print(\"Loading token data...\")\n    token_data = torch.load(DATA_PATH, map_location='cpu')\n    \n    # Extract tensors\n    input_ids = token_data[\"input_ids\"]\n    attention_mask = token_data[\"attention_mask\"]\n    print(f\"Loaded {len(input_ids)} samples\")\n    \n    # Create DataLoader\n    dataset = TensorDataset(input_ids, attention_mask)\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    # Initialize models\n    print(\"Loading teacher and student models...\")\n    teacher = BertForMaskedLM.from_pretrained(TEACHER_MODEL).to(DEVICE)\n    student = DistilBertForMaskedLM.from_pretrained(STUDENT_MODEL).to(DEVICE)\n    teacher.eval()\n    \n    # Loss functions and optimizer\n    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n    cosine_loss = nn.CosineEmbeddingLoss()\n    optimizer = AdamW(student.parameters(), lr=5e-5, weight_decay=1e-4)\n    \n    # Gradient accumulation steps\n    grad_accum_steps = 2\n    print(f\"\\nStarting distillation training for {EPOCHS} epochs\")\n    print(f\"Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * grad_accum_steps})\")\n    print(f\"Total batches: {len(dataloader)}\")\n    \n    # Training loop\n    for epoch in range(EPOCHS):\n        student.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        \n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), \n                           desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        for batch_idx, batch in progress_bar:\n            batch_input_ids, batch_attention_mask = [t.to(DEVICE) for t in batch]\n            \n            # Create masked inputs (15% masking probability)\n            masked_input_ids = batch_input_ids.clone()\n            mask_prob = torch.rand(masked_input_ids.shape, device=DEVICE)\n            \n            # Only mask non-special tokens\n            special_tokens_mask = (batch_input_ids == 0) | (batch_input_ids == 101) | (batch_input_ids == 102)\n            mask = (mask_prob < 0.15) & ~special_tokens_mask\n            masked_input_ids[mask] = 103  # DistilBERT's mask token ID\n            \n            # Teacher forward pass\n            with torch.no_grad():\n                teacher_outputs = teacher(\n                    input_ids=masked_input_ids,\n                    attention_mask=batch_attention_mask,\n                    output_hidden_states=True\n                )\n            \n            # Student forward pass with labels for proper MLM loss\n            student_outputs = student(\n                input_ids=masked_input_ids,\n                attention_mask=batch_attention_mask,\n                output_hidden_states=True,\n                labels=batch_input_ids  # Add labels for built-in MLM loss\n            )\n            \n            # Calculate losses\n            # Use log probabilities for numerical stability\n            student_log_probs = torch.nn.functional.log_softmax(student_outputs.logits, dim=-1)\n            \n            # Apply temperature scaling to teacher outputs\n            teacher_logits = teacher_outputs.logits / 2.0\n            teacher_probs = torch.nn.functional.softmax(teacher_logits, dim=-1)\n            \n            distil_loss = kl_loss(student_log_probs, teacher_probs)\n            \n            # Hidden states loss\n            student_hidden = student_outputs.hidden_states[-1]\n            teacher_hidden = teacher_outputs.hidden_states[-1]\n            \n            # Flatten hidden states for cosine loss\n            student_flat = student_hidden.view(-1, student_hidden.size(-1))\n            teacher_flat = teacher_hidden.view(-1, teacher_hidden.size(-1))\n            target = torch.ones(student_flat.size(0), device=DEVICE)\n            \n            cos_loss = cosine_loss(student_flat, teacher_flat, target)\n            \n            # Use built-in MLM loss (only calculates loss on masked tokens)\n            mlm_loss = student_outputs.loss\n            \n            # Combined loss\n            loss = (ALPHA * distil_loss + \n                    BETA * mlm_loss + \n                    GAMMA * cos_loss) / grad_accum_steps\n            \n            # Backpropagation with gradient accumulation\n            loss.backward()\n            \n            if (batch_idx + 1) % grad_accum_steps == 0:\n                torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            total_loss += loss.item() * grad_accum_steps\n            \n            # Update progress bar\n            if batch_idx % 10 == 0:\n                progress_bar.set_postfix({\n                    \"batch_loss\": f\"{loss.item() * grad_accum_steps:.4f}\",\n                    \"avg_loss\": f\"{total_loss/(batch_idx+1):.4f}\",\n                    \"d_loss\": f\"{distil_loss.item():.4f}\",\n                    \"m_loss\": f\"{mlm_loss.item():.4f}\",\n                    \"c_loss\": f\"{cos_loss.item():.4f}\"\n                })\n            \n            # Clear memory\n            del masked_input_ids, mask_prob, mask, teacher_outputs, student_outputs\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Final gradient step if needed\n        if len(dataloader) % grad_accum_steps != 0:\n            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch+1} Complete - Avg Loss: {avg_loss:.4f}\")\n    \n    # Save distilled model\n    os.makedirs(SAVE_PATH, exist_ok=True)\n    student.save_pretrained(SAVE_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL)\n    tokenizer.save_pretrained(SAVE_PATH)\n    print(f\"\\nDistilled model saved to {SAVE_PATH}\")\n    return\n\n# Run distillation training\ndistillation_training()\n\n# Step 3: Classification Fine-tuning with Fallback\ndef train_classifier():\n    # Configuration\n    MODEL_PATH = \"/kaggle/working/models/distilled_model\"\n    TRAIN_CSV = \"/kaggle/working/data/processed/train.csv\"\n    DEV_CSV = \"/kaggle/working/data/processed/dev.csv\"\n    SAVE_PATH = \"/kaggle/working/models/classification_model\"\n    BATCH_SIZE = 8\n    EPOCHS = 3\n    LEARNING_RATE = 3e-5\n    \n    # Load data\n    train_df = pd.read_csv(TRAIN_CSV)\n    dev_df = pd.read_csv(DEV_CSV)\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    \n    # Try loading distilled model, fallback to pretrained if failed\n    try:\n        model = DistilBertForSequenceClassification.from_pretrained(\n            MODEL_PATH, \n            num_labels=2,\n            ignore_mismatched_sizes=True\n        ).to(DEVICE)\n        print(\"Loaded distilled model for classification fine-tuning\")\n    except:\n        print(\"Failed to load distilled model, using pretrained as fallback\")\n        model = DistilBertForSequenceClassification.from_pretrained(\n            \"distilbert-base-uncased\", \n            num_labels=2\n        ).to(DEVICE)\n    \n    # Create datasets and dataloaders\n    train_dataset = ClassificationDataset(\n        train_df[\"text\"].tolist(), \n        train_df[\"label\"].tolist(), \n        tokenizer\n    )\n    dev_dataset = ClassificationDataset(\n        dev_df[\"text\"].tolist(), \n        dev_df[\"label\"].tolist(), \n        tokenizer\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n    \n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)*EPOCHS)\n    \n    # Training loop\n    best_f1 = 0\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        for batch in progress_bar:\n            inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].to(DEVICE)\n            \n            # Forward pass\n            outputs = model(**inputs, labels=labels)\n            loss = outputs.loss\n            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n        \n        avg_train_loss = total_loss / len(train_loader)\n        \n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in dev_loader:\n                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n                labels = batch[\"labels\"].to(DEVICE)\n                \n                outputs = model(**inputs, labels=labels)\n                preds = torch.argmax(outputs.logits, dim=1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                val_loss += outputs.loss.item()\n        \n        # Calculate metrics\n        if len(all_labels) > 0:\n            accuracy = accuracy_score(all_labels, all_preds)\n            f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            avg_val_loss = val_loss / len(dev_loader)\n            \n            print(f\"\\nEpoch {epoch+1} Evaluation:\")\n            print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n            print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n            print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n            \n            # Save best model\n            if f1 > best_f1:\n                best_f1 = f1\n                model.save_pretrained(SAVE_PATH)\n                tokenizer.save_pretrained(SAVE_PATH)\n                print(f\"New best model saved to {SAVE_PATH} with F1: {f1:.4f}\")\n        else:\n            print(f\"\\nEpoch {epoch+1} Evaluation: No valid labels to evaluate\")\n    \n    print(\"Training complete!\")\n    return\n\n# Run classification training\ntrain_classifier()\n\n# Define RationaleDataset class\nclass RationaleDataset(Dataset):\n    def __init__(self, texts, rationale_masks, tokenizer, max_len=512):\n        self.texts = texts\n        self.rationale_masks = rationale_masks\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        \n        # Get rationale mask for this sample\n        mask = self.rationale_masks[idx]\n        # Pad or truncate mask to max length\n        mask = mask[:self.max_len] + [0] * (self.max_len - len(mask))\n        \n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(mask, dtype=torch.long)\n        }\n\ndef train_rationale_model():\n    # Configuration - ONLY USE EXPERT DATA FOR RATIONALE TRAINING\n    MODEL_PATH = \"/kaggle/working/models/distilled_model\"\n    EXPERT_CSV = \"/kaggle/working/data/processed/expert.csv\"\n    SAVE_PATH = \"/kaggle/working/models/rationale_model\"\n    BATCH_SIZE = 8\n    EPOCHS = 10  # More epochs for small dataset\n    LEARNING_RATE = 3e-5\n    MAX_LEN = 512\n    \n    # Load expert data\n    print(\"Loading expert data for rationale training...\")\n    expert_df = pd.read_csv(EXPERT_CSV)\n    \n    # Convert rationale masks\n    expert_df[\"rationale_mask\"] = expert_df[\"rationale_mask\"].apply(\n        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n    )\n    \n    # Split expert data into train and validation\n    train_df, val_df = train_test_split(expert_df, test_size=0.2, random_state=42)\n    \n    # Enhanced class weight calculation\n    def calculate_class_weights(masks):\n        total_tokens = 0\n        positive_tokens = 0\n        for mask in masks:\n            # Consider only first MAX_LEN tokens\n            valid_mask = mask[:MAX_LEN]\n            total_tokens += len(valid_mask)\n            positive_tokens += sum(valid_mask)\n        \n        print(f\"Positive tokens: {positive_tokens}/{total_tokens} ({positive_tokens/total_tokens:.4%})\")\n        \n        # Handle case where there are no positive tokens\n        if positive_tokens == 0:\n            return torch.tensor([1.0, 1.0]).to(DEVICE)\n        \n        weight_positive = total_tokens / (2.0 * positive_tokens)\n        weight_negative = total_tokens / (2.0 * (total_tokens - positive_tokens))\n        \n        print(f\"Class weights - Negative: {weight_negative:.2f}, Positive: {weight_positive:.2f}\")\n        return torch.tensor([weight_negative, weight_positive]).to(DEVICE)\n    \n    # Calculate class weights using training data only\n    class_weights = calculate_class_weights(train_df[\"rationale_mask\"].tolist())\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    \n    # Create datasets and dataloaders\n    train_dataset = RationaleDataset(\n        train_df[\"text\"].tolist(),\n        train_df[\"rationale_mask\"].tolist(),\n        tokenizer\n    )\n    val_dataset = RationaleDataset(\n        val_df[\"text\"].tolist(),\n        val_df[\"rationale_mask\"].tolist(),\n        tokenizer\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    \n    # Initialize model from distilled base\n    model = DistilBertForTokenClassification.from_pretrained(\n        MODEL_PATH, \n        num_labels=2,\n        ignore_mismatched_sizes=True\n    ).to(DEVICE)\n    \n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader)*EPOCHS)\n    \n    # Loss function with class weights\n    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n    \n    # Training loop\n    best_f1 = 0\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        for batch in progress_bar:\n            inputs = {\n                'input_ids': batch['input_ids'].to(DEVICE),\n                'attention_mask': batch['attention_mask'].to(DEVICE),\n            }\n            labels = batch['labels'].to(DEVICE)\n            \n            # Forward pass\n            outputs = model(**inputs)\n            logits = outputs.logits\n            \n            # Calculate loss only on active tokens\n            active_loss = inputs['attention_mask'].view(-1) == 1\n            active_logits = logits.view(-1, 2)[active_loss]\n            active_labels = labels.view(-1)[active_loss]\n            \n            if active_labels.numel() > 0:\n                loss = loss_fn(active_logits, active_labels)\n            else:\n                loss = torch.tensor(0.0, requires_grad=True).to(DEVICE)\n            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            progress_bar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n            })\n        \n        avg_train_loss = total_loss / len(train_loader)\n        \n        # Evaluation\n        model.eval()\n        all_preds = []\n        all_labels = []\n        val_loss = 0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {\n                    'input_ids': batch['input_ids'].to(DEVICE),\n                    'attention_mask': batch['attention_mask'].to(DEVICE),\n                }\n                labels = batch['labels'].to(DEVICE)\n                \n                outputs = model(**inputs)\n                logits = outputs.logits\n                \n                # Get predictions\n                active_mask = inputs['attention_mask'].view(-1) == 1\n                active_logits = logits.view(-1, 2)[active_mask]\n                active_labels = labels.view(-1)[active_mask]\n                \n                preds = torch.argmax(active_logits, dim=-1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(active_labels.cpu().numpy())\n                val_loss += loss_fn(active_logits, active_labels).item() if active_labels.numel() > 0 else 0\n        \n        # Calculate metrics\n        if len(all_labels) > 0:\n            accuracy = accuracy_score(all_labels, all_preds)\n            f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n            avg_val_loss = val_loss / len(val_loader)\n            \n            print(f\"\\nEpoch {epoch+1} Evaluation:\")\n            print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n            print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n            \n            # Save best model\n            if f1 > best_f1:\n                best_f1 = f1\n                model.save_pretrained(SAVE_PATH)\n                tokenizer.save_pretrained(SAVE_PATH)\n                print(f\"New best model saved to {SAVE_PATH} with F1: {f1:.4f}\")\n        else:\n            print(f\"\\nEpoch {epoch+1} Evaluation: No valid labels to evaluate\")\n    \n    # Always save final model\n    if not os.path.exists(SAVE_PATH):\n        model.save_pretrained(SAVE_PATH)\n        tokenizer.save_pretrained(SAVE_PATH)\n        print(f\"Final model saved to {SAVE_PATH}\")\n    \n    print(\"Training complete!\")\n    return model\n\n# Run rationale training\nrationale_model = train_rationale_model()\n\n# Step 5: Enhanced Inference Demo\nclass InferenceSystem:\n    def __init__(self):\n        # Configuration\n        self.CLASSIFIER_PATH = \"/kaggle/working/models/classification_model\"\n        self.RATIONALE_PATH = \"/kaggle/working/models/rationale_model\"\n        self.DISTILLED_PATH = \"/kaggle/working/models/distilled_model\"\n        \n        # Load components\n        self.tokenizer = self._load_tokenizer()\n        self.classifier = self._load_classifier()\n        self.rationale_extractor = self._load_rationale_extractor()\n    \n    def _load_tokenizer(self):\n        # Try rationale path first, then fallback to classifier\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.RATIONALE_PATH)\n            print(\"Loaded tokenizer from rationale model\")\n            return tokenizer\n        except:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(self.CLASSIFIER_PATH)\n                print(\"Loaded tokenizer from classification model\")\n                return tokenizer\n            except:\n                tokenizer = AutoTokenizer.from_pretrained(self.DISTILLED_PATH)\n                print(\"Loaded tokenizer from distilled model\")\n                return tokenizer\n    \n    def _load_classifier(self):\n        try:\n            return DistilBertForSequenceClassification.from_pretrained(\n                self.CLASSIFIER_PATH\n            ).to(DEVICE).eval()\n        except Exception as e:\n            print(f\"Error loading classifier: {e}\")\n            return None\n    \n    def _load_rationale_extractor(self):\n        try:\n            model = DistilBertForTokenClassification.from_pretrained(\n                self.RATIONALE_PATH\n            ).to(DEVICE).eval()\n            print(\"Loaded rationale extraction model\")\n            return model\n        except Exception as e:\n            print(f\"Could not load rationale model: {e}\")\n            print(\"Using classification model with attention fallback\")\n            return None\n    \n    def predict(self, text):\n        # Tokenize input\n        inputs = self.tokenizer(\n            text, \n            return_tensors=\"pt\", \n            max_length=512, \n            padding=\"max_length\", \n            truncation=True\n        ).to(DEVICE)\n        \n        with torch.no_grad():\n            # Predict outcome\n            if self.classifier is None:\n                raise RuntimeError(\"Classifier model failed to load\")\n                \n            clf_output = self.classifier(**inputs)\n            label = clf_output.logits.argmax(-1).item()\n            outcome = \"ALLOWED\" if label == 1 else \"DISMISSED\"\n            \n            # Extract rationale\n            if self.rationale_extractor:\n                token_output = self.rationale_extractor(**inputs)\n                mask = token_output.logits.argmax(-1).squeeze().cpu().numpy()\n            else:\n                # Fallback: use attention weights from classifier\n                outputs = self.classifier(**inputs, output_attentions=True)\n                attentions = torch.stack(outputs.attentions).mean(0).mean(1)[0, 0].cpu().numpy()\n                mask = (attentions > attentions.mean()).astype(int)\n            \n            tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().cpu().numpy())\n            attention_mask = inputs[\"attention_mask\"].squeeze().cpu().numpy()\n            \n            # Filter out special tokens and create highlighted text\n            highlighted_tokens = []\n            for i, (tok, m) in enumerate(zip(tokens, mask)):\n                # Skip special tokens and padding\n                if tok in [self.tokenizer.cls_token, \n                          self.tokenizer.sep_token, \n                          self.tokenizer.pad_token] or attention_mask[i] == 0:\n                    continue\n                    \n                # Clean up token representation\n                if tok.startswith(\"##\"):\n                    tok = tok[2:]\n                    if highlighted_tokens:\n                        highlighted_tokens[-1] += tok\n                        continue\n                \n                if m == 1:\n                    highlighted_tokens.append(f\"[{tok}]\")\n                else:\n                    highlighted_tokens.append(tok)\n            \n            # Convert to readable text\n            rationale_text = \" \".join(highlighted_tokens)\n            # Clean up spacing around punctuation\n            rationale_text = re.sub(r'\\s+([.,;:!?])', r'\\1', rationale_text)\n            rationale_text = re.sub(r'\\[\\s+', '[', rationale_text)\n            rationale_text = re.sub(r'\\s+\\]', ']', rationale_text)\n        \n        return outcome, rationale_text\n\n# Test with sample case\nsample_case = \"\"\"\nThe appellant was charged under Section 302 of the Indian Penal Code for the murder of his neighbor.\nEvidence shows the accused was present at the crime scene, and fingerprints match those found on the weapon.\nHowever, the defense argues there was no motive and the forensic evidence was mishandled by police.\nThe prosecution maintains the circumstantial evidence is sufficient for conviction.\n\"\"\"\n\nprint(\"Initializing inference system...\")\ninference_system = InferenceSystem()\n\nprint(\"\\nRunning sample prediction...\")\noutcome, rationale = inference_system.predict(sample_case)\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Predicted Outcome: {outcome}\")\nprint(\"\\nExtracted Rationale (key phrases in brackets):\")\nprint(\"-\"*60)\nprint(rationale)\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T13:26:03.973096Z","iopub.execute_input":"2025-07-22T13:26:03.973926Z","iopub.status.idle":"2025-07-22T14:02:16.766699Z","shell.execute_reply.started":"2025-07-22T13:26:03.973892Z","shell.execute_reply":"2025-07-22T14:02:16.765934Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading CJPE dataset...\nDataset loaded successfully!\nTokenizing data...\nProcessing expert annotations for rationale extraction...\n","output_type":"stream"},{"name":"stderr","text":"Expert rationale masks: 100%|██████████| 56/56 [00:00<00:00, 86.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Preprocessing complete! Files saved to /kaggle/working/data/processed\nExpert rationale coverage: 33.5658%\nLoading token data...\nLoaded 5082 samples\nLoading teacher and student models...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"\nStarting distillation training for 1 epochs\nBatch size: 8 (effective: 16)\nTotal batches: 636\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 636/636 [14:21<00:00,  1.36s/it, batch_loss=29.8640, avg_loss=38.5053, d_loss=58.5412, m_loss=1.4276, c_loss=0.2237]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Complete - Avg Loss: 38.4480\n\nDistilled model saved to /kaggle/working/models/distilled_model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/models/distilled_model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Loaded distilled model for classification fine-tuning\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 636/636 [05:28<00:00,  1.94it/s, loss=0.6919, lr=2.25e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Evaluation:\nTrain Loss: 0.6642, Val Loss: 0.6994\nAccuracy: 0.5062, F1: 0.0343\nPrecision: 0.8800, Recall: 0.0175\nNew best model saved to /kaggle/working/models/classification_model with F1: 0.0343\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 636/636 [05:28<00:00,  1.94it/s, loss=0.6399, lr=7.50e-06]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Evaluation:\nTrain Loss: 0.6399, Val Loss: 0.7036\nAccuracy: 0.5627, F1: 0.3754\nPrecision: 0.6613, Recall: 0.2621\nNew best model saved to /kaggle/working/models/classification_model with F1: 0.3754\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 636/636 [05:28<00:00,  1.94it/s, loss=0.4454, lr=0.00e+00]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Evaluation:\nTrain Loss: 0.5935, Val Loss: 0.7024\nAccuracy: 0.5850, F1: 0.5144\nPrecision: 0.6223, Recall: 0.4384\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at /kaggle/working/models/distilled_model and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"New best model saved to /kaggle/working/models/classification_model with F1: 0.5144\nTraining complete!\nLoading expert data for rationale training...\nPositive tokens: 7361/22528 (32.6749%)\nClass weights - Negative: 0.74, Positive: 1.53\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 6/6 [00:02<00:00,  2.23it/s, loss=0.8230, lr=2.93e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Evaluation:\nTrain Loss: 0.7487, Val Loss: 0.6922\nAccuracy: 0.6337, F1: 0.1383, Precision: 0.5723, Recall: 0.0787\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.1383\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 6/6 [00:02<00:00,  2.27it/s, loss=0.7122, lr=2.71e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Evaluation:\nTrain Loss: 0.6875, Val Loss: 0.6453\nAccuracy: 0.5339, F1: 0.5978, Precision: 0.4412, Recall: 0.9266\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.5978\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, loss=0.6684, lr=2.38e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Evaluation:\nTrain Loss: 0.6566, Val Loss: 0.6192\nAccuracy: 0.6477, F1: 0.5623, Precision: 0.5249, Recall: 0.6054\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 6/6 [00:02<00:00,  2.27it/s, loss=0.5257, lr=1.96e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Evaluation:\nTrain Loss: 0.6069, Val Loss: 0.5814\nAccuracy: 0.6500, F1: 0.5965, Precision: 0.5241, Recall: 0.6920\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, loss=0.6070, lr=1.50e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Evaluation:\nTrain Loss: 0.5876, Val Loss: 0.5451\nAccuracy: 0.6476, F1: 0.6265, Precision: 0.5187, Recall: 0.7910\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.6265\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, loss=0.4812, lr=1.04e-05]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 Evaluation:\nTrain Loss: 0.5305, Val Loss: 0.5422\nAccuracy: 0.6766, F1: 0.6152, Precision: 0.5540, Recall: 0.6916\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, loss=0.4328, lr=6.18e-06]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 Evaluation:\nTrain Loss: 0.4893, Val Loss: 0.5259\nAccuracy: 0.6803, F1: 0.6258, Precision: 0.5562, Recall: 0.7154\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 6/6 [00:02<00:00,  2.26it/s, loss=0.4798, lr=2.86e-06]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 Evaluation:\nTrain Loss: 0.4690, Val Loss: 0.5208\nAccuracy: 0.6827, F1: 0.6275, Precision: 0.5591, Recall: 0.7150\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.6275\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 6/6 [00:02<00:00,  2.26it/s, loss=0.5694, lr=7.34e-07]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 Evaluation:\nTrain Loss: 0.4620, Val Loss: 0.5165\nAccuracy: 0.6836, F1: 0.6342, Precision: 0.5583, Recall: 0.7340\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.6342\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 6/6 [00:02<00:00,  2.26it/s, loss=0.4133, lr=0.00e+00]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 Evaluation:\nTrain Loss: 0.4444, Val Loss: 0.5162\nAccuracy: 0.6841, F1: 0.6346, Precision: 0.5589, Recall: 0.7340\nNew best model saved to /kaggle/working/models/rationale_model with F1: 0.6346\nTraining complete!\nInitializing inference system...\nLoaded tokenizer from rationale model\nLoaded rationale extraction model\n\nRunning sample prediction...\n\n============================================================\nPredicted Outcome: DISMISSED\n\nExtracted Rationale (key phrases in brackets):\n------------------------------------------------------------\n[the] [app]ellant [was] [charged] [under] [section] [302] [of] [the] [indian] [penal] [code] [for] [the] [murder] [of] [his] [neighbor] [.] [evidence] [shows] [the] [accused] [was] [present] [at] [the] [crime] [scene] [,] [and] [fingerprints] [match] [those] [found] [on] [the] [weapon] [.] [however] [,] [the] [defense] [argues] [there] [was] [no] [motive] [and] [the] [forensic] [evidence] [was] [mis]handled [by] [police] [.] [the] [prosecution] [maintains] [the] [ci]rcumstantial [evidence] [is] [sufficient] [for] [conviction] [.]\n============================================================\n","output_type":"stream"}],"execution_count":4}]}